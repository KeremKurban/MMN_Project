{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologyData:\n",
    "    def __init__(self, master_path='regions/', json_save_dir='morph_metadata/'):\n",
    "        self.master_path = master_path\n",
    "        self.json_save_dir = json_save_dir\n",
    "        self.df = None\n",
    "        self.morpho_df = None\n",
    "        self.as_morph = None\n",
    "        self.as_metadata = None\n",
    "\n",
    "    @staticmethod\n",
    "    def check_swc(curdir):\n",
    "        for i in os.path.listdir(curdir):\n",
    "            if i.endswith('swc'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def check_metadata_exists(self, name):\n",
    "        for i in os.listdir(self.json_save_dir):\n",
    "            if i == f'{name}.json':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fetch_metadata(self):\n",
    "        '''\n",
    "        STEP 1: Fetch metadata from neuromorpho.org using the name of the swc file (assumed its downloaded)\n",
    "        See neuromorpho to download all the data you want, then process them here\n",
    "        '''\n",
    "\n",
    "        print(f'Extracting data from {self.master_path}')\n",
    "        for i in os.listdir(f'{self.master_path}'):\n",
    "            name = i.split('.')[0]\n",
    "            if self.check_metadata_exists(name):\n",
    "                continue\n",
    "            command = f\"sh get_from_name.sh {name} {self.json_save_dir}\"\n",
    "            process = Popen(command, shell=True, stdout=PIPE, stderr=STDOUT)\n",
    "\n",
    "            with process.stdout:\n",
    "                for line in iter(process.stdout.readline, b''):\n",
    "                    print(line.decode(\"utf-8\").strip())\n",
    "\n",
    "\n",
    "    def combine_raw_metadata(self):\n",
    "            '''\n",
    "            STEP 2: After fetching metadata in separate json files, combine them into a single csv file before preprocessing\n",
    "            '''\n",
    "            dfs = []\n",
    "            for file in os.listdir(self.json_save_dir):\n",
    "                with open(f'{self.json_save_dir}/{file}') as f:\n",
    "                    json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                dfs.append(json_data)\n",
    "\n",
    "            df = pd.concat(dfs, sort=False)  # or sort=True depending on your needs\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            df.to_csv('morpho_metadata_raw.csv')\n",
    "            return df\n",
    "\n",
    "    def clean_metadata(self,df):\n",
    "            '''\n",
    "            STEP 3: Clean the metadata\n",
    "            Reasonings for cleaning are explained in comments per section\n",
    "            '''\n",
    "            # experiment_condition\n",
    "            ## Some column variables are depicted as lists although there is 1 element. Groupby does not like that so lets take the 1st elem\n",
    "            df['experiment_condition'] = df['experiment_condition'].apply(lambda x: x[0])\n",
    "\n",
    "            # (Skipped) Reference_doi: Some list columns have more than 1 value or No value. Lets examine those\n",
    "            #refdoi = df.copy().reference_doi\n",
    "            #refdoi.dropna(inplace=True)\n",
    "\n",
    "            # Species \n",
    "            # Somehow morphoneuro name query returns non mouse morphologies. Filter them out\n",
    "            # lets also keep track of what we have dropped\n",
    "            shouldnt_be_in_df = df[df.species!='mouse']\n",
    "            df.drop(df[df.species!='mouse'].index,inplace=True) # somehow some queries returned non mouse entries\n",
    "\n",
    "            # Domain: remove all except dsa and dsna (n=3 cells only)\n",
    "            temp_df = df[df.domain != 'Dendrites, Soma, Axon']\n",
    "            temp_df = temp_df[temp_df.domain != 'Dendrites, Soma, No Axon']\n",
    "            df.drop(temp_df.index,inplace=True)\n",
    "            shouldnt_be_in_df = pd.concat([shouldnt_be_in_df,temp_df])\n",
    "\n",
    "            # Experiment-Control Group Difference\n",
    "            df_control_group = df[df.experiment_condition == 'Control']\n",
    "            df_experiment_group = df[df.experiment_condition != 'Control']\n",
    "\n",
    "            df = df.reset_index(drop=True)\n",
    "            df2 = df.copy()\n",
    "\n",
    "            # Brain regions: Convert from list to 5 level categories\n",
    "            max_level_in_data = np.max([len(i) for i in df2['brain_region']])\n",
    "            cols_brainreg = [f'brain_region_{i}' for i in range(1,max_level_in_data+1)]\n",
    "\n",
    "            reg_df = pd.DataFrame(index=df2.index,columns=cols_brainreg)\n",
    "\n",
    "            for idx,i in df2['brain_region'].items():\n",
    "                len_i = len(i)\n",
    "                for idx2,j in enumerate(i):\n",
    "                    reg_df.iloc[idx,idx2] = j\n",
    "\n",
    "            df2 = df2.join(reg_df)\n",
    "\n",
    "            # Cell types : Similar to brain regions but we have to do it for each cell type\n",
    "            max_level_in_cell_type = np.max([len(i) for i in df['cell_type']])\n",
    "            cols_celltype = [f'cell_type_{i}' for i in range(1,max_level_in_cell_type+1)]\n",
    "            cell_type_df = pd.DataFrame(index=df2.index,columns=cols_celltype)\n",
    "\n",
    "            for idx,i in df2['cell_type'].items():\n",
    "                for idx2,j in enumerate(i):\n",
    "                    cell_type_df.iloc[idx,idx2] = j\n",
    "\n",
    "            df2 = df2.join(cell_type_df)\n",
    "\n",
    "            # Reorder columns\n",
    "            new_col_order = ['neuron_id', 'neuron_name', 'archive', 'note', 'age_scale', 'gender',\n",
    "                'age_classification', 'brain_region','brain_region_1', 'brain_region_2', 'brain_region_3', 'brain_region_4',\n",
    "                'brain_region_5', 'cell_type','cell_type_1','cell_type_2','cell_type_3','cell_type_4','cell_type_5',\n",
    "                'cell_type_6', 'species', 'strain',\n",
    "                'scientific_name', 'stain', 'experiment_condition', 'protocol',\n",
    "                'slicing_direction', 'reconstruction_software', 'objective_type',\n",
    "                'original_format', 'domain', 'attributes', 'magnification',\n",
    "                'upload_date', 'deposition_date', 'shrinkage_reported',\n",
    "                'shrinkage_corrected', 'reported_value', 'reported_xy', 'reported_z',\n",
    "                'corrected_value', 'corrected_xy', 'corrected_z', 'soma_surface',\n",
    "                'surface', 'volume', 'slicing_thickness', 'min_age', 'max_age',\n",
    "                'min_weight', 'max_weight', 'png_url', 'reference_pmid',\n",
    "                'reference_doi', 'physical_Integrity', '_links.self.href',\n",
    "                '_links.measurements.href', '_links.persistence_vector.href']\n",
    "            df2 = df2[new_col_order]\n",
    "\n",
    "            # filter out non-hippocampal regions\n",
    "            shouldnt_be_in_df2 = df2[df2.brain_region_1=='retina']\n",
    "            shouldnt_be_in_df2 = pd.concat([shouldnt_be_in_df2,df2[df2.brain_region_1=='neocortex']])\n",
    "\n",
    "            df2.drop(shouldnt_be_in_df2.index,inplace=True)\n",
    "\n",
    "            na_cols = df2.columns[df2.isna().all().values]\n",
    "            df2.drop(na_cols,axis=1,inplace=True)\n",
    "\n",
    "            # fill empty values with 'Unknown'\n",
    "            df2.reset_index(inplace=True,drop=True)\n",
    "            drop_idx = np.where(df2.shrinkage_corrected=='')[0]\n",
    "            df2.loc[drop_idx,'shrinkage_corrected'] = 'Unknown'\n",
    "\n",
    "            drop_idx2 = np.where(df2.shrinkage_reported=='')[0]\n",
    "            df2.loc[drop_idx2,'shrinkage_reported'] = 'Unknown'\n",
    "\n",
    "            df2.shrinkage_reported = df2.shrinkage_reported.str.title()\n",
    "            df2.shrinkage_corrected = df2.shrinkage_corrected.str.title()\n",
    "\n",
    "            # some morphologies are from control and some experiment group. \n",
    "            # Since each process might affect the morphology, we will keep them separate\n",
    "            df2_control_group = df2[df2.experiment_condition == 'Control']\n",
    "            df2_experiment_group = df2[df2.experiment_condition != 'Control']\n",
    "            df2_control_group.reset_index(inplace=True,drop=True)\n",
    "            df2_experiment_group.reset_index(inplace=True,drop=True)\n",
    "            df2_control_group.to_csv('morpho_metadata_controls.csv')\n",
    "            df2_experiment_group.to_csv('morpho_metadata_experiments.csv')\n",
    "\n",
    "            self.cleaned_metadata = df2\n",
    "            return df2\n",
    "\n",
    "\n",
    "    def save(self,save_path='morpho_metadata_processed.csv'):\n",
    "        '''\n",
    "        STEP 4: Save the cleaned metadata\n",
    "        '''\n",
    "        self.cleaned_metadata.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ../data/morphologies/swc30/\n",
      "--2023-05-16 19:33:28--  http://neuromorpho.org/api/neuron/data/metadata/morph_features/\n",
      "Resolving bbpproxy.epfl.ch (bbpproxy.epfl.ch)... 192.33.211.34\n",
      "Connecting to bbpproxy.epfl.ch (bbpproxy.epfl.ch)|192.33.211.34|:80... connected.\n",
      "Proxy request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://neuromorpho.org/api/neuron/data/metadata/morph_features/ [following]\n",
      "--2023-05-16 19:33:29--  https://neuromorpho.org/api/neuron/data/metadata/morph_features/\n",
      "Connecting to bbpproxy.epfl.ch (bbpproxy.epfl.ch)|192.33.211.34|:80... connected.\n",
      "Proxy request sent, awaiting response... 404 Not Found\n",
      "2023-05-16 19:33:29 ERROR 404: Not Found.\n",
      "\n",
      "get_from_name.sh: line 2: /../data/metadata/morph_features/.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "morph_data = MorphologyData(master_path='../data/morphologies/swc30/',json_save_dir='../data/metadata/morph_features/')\n",
    "morph_data.fetch_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmorph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_raw_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# morph_data.clean_metadata()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# morph_data.save()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mMorphologyData.combine_raw_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_save_dir):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 49\u001b[0m         json_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(json_data)\n\u001b[1;32m     52\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# or sort=True depending on your needs\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/deploy/externals/2021-01-06/linux-rhel7-x86_64/gcc-9.3.0/python-3.8.3-suxrst/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/deploy/externals/2021-01-06/linux-rhel7-x86_64/gcc-9.3.0/python-3.8.3-suxrst/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/deploy/externals/2021-01-06/linux-rhel7-x86_64/gcc-9.3.0/python-3.8.3-suxrst/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "morph_data.combine_raw_metadata()\n",
    "# morph_data.clean_metadata()\n",
    "# morph_data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            dfs = []\n",
    "            for file in os.listdir(self.json_save_dir):\n",
    "                with open(f'{self.json_save_dir}/{file}') as f:\n",
    "                    json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                dfs.append(json_data)\n",
    "\n",
    "            df = pd.concat(dfs, sort=False)  # or sort=True depending on your needs\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            df.to_csv('morpho_metadata_raw.csv')\n",
    "            return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2022",
   "language": "python",
   "name": "venv2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
